{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mizo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import email.parser\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input source directory TR: C:\\Users\\Mizo\\Documents\\ICYBM121-p2\\data\\TR\n"
     ]
    }
   ],
   "source": [
    "tr_src_dir = input('Input source directory TR: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input source directory TT: C:\\Users\\Mizo\\Documents\\ICYBM121-p2\\data\\TT\n"
     ]
    }
   ],
   "source": [
    "tt_src_dir = input('Input source directory TT: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input destination directory: C:\\Users\\Mizo\\Documents\\ICYBM121-p2\n"
     ]
    }
   ],
   "source": [
    "dest_dir = input('Input destination directory: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lexical analysis (tokenization)\n",
    "headers, attachments and HTML tags are stripped\n",
    "result only contains the e-mail body and subject line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_analysis(mail):\n",
    "    m = open(mail, 'rb')\n",
    "    try:\n",
    "        msg = email.message_from_binary_file(m)\n",
    "        subject = msg.get('subject')\n",
    "        multipart_counter = 0\n",
    "        no_txt = True\n",
    "        while no_txt and msg.is_multipart():\n",
    "            \n",
    "            msg = msg.get_payload(multipart_counter)\n",
    "            if isinstance(msg.get_payload(), str):\n",
    "                no_txt = False \n",
    "            multipart_counter += 1\n",
    "        body = msg.get_payload()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('error parsing mail')\n",
    "        body = 'not available'\n",
    "        subject = 'not available'\n",
    "        \n",
    "        \n",
    "    return subject, body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract field from mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_of_mail(mail,field):\n",
    "    m = open(mail, 'rb')\n",
    "    try:\n",
    "        msg = email.message_from_binary_file(m)\n",
    "        extracted_field = msg.get(field)\n",
    "    except Exception as e:\n",
    "        extracted_field = 'not available'\n",
    "        \n",
    "    return extracted_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(text):\n",
    "    new_text = text\n",
    "    # list of english stopwords\n",
    "    stop_words = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "    for sw in stop_words:\n",
    "        if sw in text.lower():\n",
    "            word = ' ' + sw + ' '\n",
    "            new_text = text.lower().replace(word, ' ')\n",
    "        text = new_text\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming\n",
    "source: https://www.geeksforgeeks.org/python-stemming-words-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " def stemming(text):\n",
    "    new_text = ''\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "     \n",
    "    for w in words: \n",
    "        new_text += ps.stem(w) + ' ' \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_word_removal(text):\n",
    "    new_text = ''\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isalpha():\n",
    "            new_text += word + ' '\n",
    "\n",
    "    return new_text        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1. Content Based Spam E-mail Filtering \n",
    "by Liu et Moh, San Jose State University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove HTML tags\n",
    "source: https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update word frequency in given dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_frequency_count(text, class_dict):\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        w = word.lower()\n",
    "        if w in class_dict.keys():\n",
    "            class_dict[w] += 1\n",
    "        else:\n",
    "            class_dict[w] = 1\n",
    "    return class_dict\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up the classification of the given mail of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_classification_for_training_set(mail):\n",
    "    classification = ''\n",
    "    number = mail.replace('TRAIN_', '').replace('.eml','')\n",
    "    with open('./data/spam-mail.tr.label') as lf:\n",
    "        lines = lf.readlines()\n",
    "        for line in lines:\n",
    "            [mail_nr, c] = line.split(',')\n",
    "            if mail_nr == number:\n",
    "                classification = c\n",
    "    return classification\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    " def training_algorithm_1(tr_src_dir):\n",
    "    spam_dict = {}\n",
    "    ham_dict = {}\n",
    "    if os.path.isdir(tr_src_dir):\n",
    "        for mail in os.listdir(tr_src_dir):\n",
    "            mail_path = '{}/{}'.format(tr_src_dir, mail)\n",
    "            _, body = lexical_analysis(mail_path)\n",
    "            if not isinstance(body,list):\n",
    "                body = remove_html_tags(body)\n",
    "                body = stemming(body)\n",
    "                body = non_word_removal(body)\n",
    "            \n",
    "                classification = lookup_classification_for_training_set(mail)\n",
    "                classification = classification.replace('\\n', '')\n",
    "                if int(classification) == 0:\n",
    "                    spam_dict = update_frequency_count(body, spam_dict)\n",
    "                else:\n",
    "                    ham_dict = update_frequency_count(body, ham_dict)\n",
    "\n",
    "    \n",
    "    threshold = 0\n",
    "    spam_dict_Copy = copy.deepcopy(spam_dict)\n",
    "    for word in spam_dict:\n",
    "        count = spam_dict[word]\n",
    "        if count < threshold:\n",
    "            del spam_dict_Copy[word]\n",
    "    \n",
    "    ham_dict_Copy = copy.deepcopy(ham_dict)\n",
    "    for word in ham_dict:\n",
    "        count = ham_dict[word]\n",
    "        if count < threshold:\n",
    "            del ham_dict_Copy[word]\n",
    "    \n",
    "    return spam_dict_Copy, ham_dict_Copy\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_algorithm_1(spam_dict, ham_dict, threshold, tt_src_dir):\n",
    "    spam_num = 779\n",
    "    ham_num = 1721\n",
    "    fd = open('spam-mail.tt.label', 'w')\n",
    "    fd.write('Id,Prediction')\n",
    "    fd.close()\n",
    "    if os.path.isdir(tt_src_dir):\n",
    "        for mail in os.listdir(tt_src_dir):\n",
    "            classification = 1\n",
    "            mail_path = '{}/{}'.format(tt_src_dir, mail)\n",
    "            _, body = lexical_analysis(mail_path)\n",
    "            if not isinstance(body,list):\n",
    "                body = remove_html_tags(body)\n",
    "                body = stemming(body)\n",
    "                body = non_word_removal(body)\n",
    "                prob_spam = 1.0\n",
    "                prob_ham = 1.0\n",
    "                words = word_tokenize(body)\n",
    "                for word in words:\n",
    "                    w = word.lower()\n",
    "                    if w in spam_dict.keys() and w in ham_dict.keys():\n",
    "                        spam_count = spam_dict[w]\n",
    "                        ham_count = ham_dict[w]\n",
    "                        p1 = (spam_count / spam_num) / ((spam_count / spam_num) + (ham_count / ham_num))\n",
    "                        p2 = 1 - p1\n",
    "                        prob_spam = prob_spam * p1\n",
    "                        prob_ham = prob_ham * p2\n",
    "                if prob_spam - prob_ham > threshold:\n",
    "                    classification = 0\n",
    "                else:\n",
    "                    classification = 1\n",
    "            email_id = mail.replace('TEST_', '').replace('.eml','')\n",
    "            fd = open('spam-mail.tt.label', 'a')\n",
    "            fd.write('\\n%d,%d' % (int(email_id), int(classification)))\n",
    "            fd.close()\n",
    "            \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n"
     ]
    }
   ],
   "source": [
    "spam_dict, ham_dict = training_algorithm_1(tr_src_dir)\n",
    "testing_algorithm_1(spam_dict, ham_dict, 0, tt_src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 2. Spam filtering by anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_algorithm_2(tr_src_dir):\n",
    "    train_set = []\n",
    "    if os.path.isdir(tr_src_dir):\n",
    "        for mail in os.listdir(tr_src_dir):\n",
    "            mail_path = '{}/{}'.format(tr_src_dir, mail)\n",
    "            \n",
    "            #preprocessing of mail\n",
    "            #only use body\n",
    "            body = lexical_analysis(mail_path)\n",
    "            content = str(body)\n",
    "                \n",
    "            train_set.append(content)\n",
    "            \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(train_set)\n",
    "        \n",
    "        clf = LocalOutlierFactor(n_neighbors=20)\n",
    "        clf.fit(X)\n",
    "        \n",
    "    return clf #LOF\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_algorithm_2(tt_src_dir, LOF):\n",
    "    fd = open('spam-mail.tt.LOF.label', 'w')\n",
    "    fd.write('Id,Prediction')\n",
    "    fd.close()\n",
    "    test_set = []\n",
    "    if os.path.isdir(tt_src_dir):\n",
    "        for mail in os.listdir(tt_src_dir):\n",
    "            mail_path = '{}/{}'.format(tt_src_dir, mail)\n",
    "            \n",
    "            #preprocessing of mail\n",
    "            #only use body\n",
    "            _, body = lexical_analysis(mail_path)\n",
    "            content = str(body)\n",
    "                \n",
    "            test_set.append(content)\n",
    "        \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(test_set)  \n",
    "        \n",
    "        res = LOF.fit_predict(X)\n",
    "        \n",
    "        fd = open('spam-mail.tt.LOF.label', 'a')\n",
    "        for i in range(len(res)):\n",
    "            if res[i] == -1:\n",
    "                fd.write('\\n%d,%d' % ( i+1 , 0))\n",
    "            else :\n",
    "                fd.write('\\n%d,%d' % ( i+1 , 1))\n",
    "        fd.close()\n",
    "        return res\n",
    "   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mizo\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n",
      "list index out of range\n",
      "error parsing mail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mizo\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1827\n",
      "{-1: 183, 1: 1644}\n"
     ]
    }
   ],
   "source": [
    "LOF = training_algorithm_2(tr_src_dir)\n",
    "res = testing_algorithm_2(tt_src_dir, LOF)\n",
    "print(len(res))\n",
    "unique, counts = np.unique(res, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mails(src_dir, dest_dir):\n",
    "    if os.path.isdir(src_dir):\n",
    "        for mail in os.listdir(src_dir):\n",
    "            mail_path = '{}/{}'.format(src_dir, mail)\n",
    "            print(mail_path)\n",
    "            subject, body = lexical_analysis(mail_path)\n",
    "            print(body)\n",
    "    else:\n",
    "        print('no source dir')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def svmTest(tr_src_dir, tt_src_dir):\n",
    "    dico, classificationList = prepare_Dico(tr_src_dir)\n",
    "    TR_matrix = features_matrix(tr_src_dir, dico, 2500) \n",
    "    clf = SVC()\n",
    "    print(len(TR_matrix))\n",
    "    print(len(classificationList))\n",
    "    clf.fit(TR_matrix , classificationList)\n",
    "    \n",
    "    #dico, classificationList = prepare_Dico(tt_src_dir, False)\n",
    "    TT_matrix = features_matrix(tt_src_dir, dico, 1827) \n",
    "    res = clf.predict(TT_matrix)\n",
    "    print(np.count_nonzero(res))\n",
    "    print(\"res size : \", len(res))\n",
    "    fd = open('spam-mail.tt.label', 'a')\n",
    "    for i in range(len(res)):\n",
    "        \n",
    "        fd.write('\\n%d,%d' % ( i+1 , res[i]))\n",
    "    fd.close()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Dico(tr_src_dir, doClassificationList = True):\n",
    "    dico = {}\n",
    "    classificationList = []\n",
    "    if os.path.isdir(tr_src_dir):\n",
    "        for mail in os.listdir(tr_src_dir):\n",
    "            mail_path = '{}/{}'.format(tr_src_dir, mail)\n",
    "            _, body = lexical_analysis(mail_path)\n",
    "            if not isinstance(body,list):\n",
    "                body = remove_html_tags(body)\n",
    "                body = stemming(body)\n",
    "                body = non_word_removal(body)\n",
    "                dico = update_frequency_count(body, dico)\n",
    "            if doClassificationList:\n",
    "                classification = lookup_classification_for_training_set(mail)\n",
    "                classification = classification.replace('\\n', '')\n",
    "                if int(classification) == 0:\n",
    "                    classificationList.append(0)\n",
    "                else:\n",
    "                    classificationList.append(1)\n",
    "    threshold = 10\n",
    "    dicoCopy = copy.deepcopy(dico)\n",
    "    print(\"dico size before : \", len(dico))\n",
    "    \n",
    "    for word in dico:\n",
    "        count = dico[word]\n",
    "        if count < threshold:\n",
    "            del dicoCopy[word]\n",
    "    \n",
    "    print(\"dico size after : \", len(dicoCopy))\n",
    "    return dicoCopy, classificationList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_matrix(tr_src_dir, dico, size):\n",
    "    id = 0\n",
    "    \n",
    "    matrix = np.zeros((size,len(dico)))\n",
    "    enumerateDico = []\n",
    "    for i, j in enumerate(dico):\n",
    "        enumerateDico.append((i,j))\n",
    "    for mail in os.listdir(tr_src_dir):\n",
    "        mail_path = '{}/{}'.format(tr_src_dir, mail)\n",
    "        _, body = lexical_analysis(mail_path)\n",
    "        if not isinstance(body,list):\n",
    "            body = body.split()\n",
    "            for word in body:\n",
    "                for i, j in enumerateDico:\n",
    "                    if str(j) == word:\n",
    "                        matrix[id,i] += 1 \n",
    "        \n",
    "            \n",
    "        id += 1\n",
    "    print(\"id :\",id )\n",
    "    print(\"count :\",np.count_nonzero(matrix))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svmTest(tr_src_dir, tt_src_dir)\n",
    "dico, classification = prepare_Dico(tr_src_dir)\n",
    "features_matrix(tr_src_dir, dico, 2145) \n",
    "spam_dict, ham_dict, threshold = training_algorithm_1(tr_src_dir)\n",
    "print(threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
